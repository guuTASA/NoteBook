# N卡伴你更好深度学习！



## 2017-11-29

### 0-

https://www.cnblogs.com/neopenx/p/4480701.html

### 1-ReLU

ReLu为网络引入了大量的稀疏性，加速了复杂特征解离。非饱和的宽广映射空间，加速了特征学习。

### 2-重叠结构

| Epoch/Pooling       | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |
| ------------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Max+Avg+Avg（默认)(最好) | 44%  | 37%  | 34%  | 30%  | 30%  | 30%  | 30%  | 30%  | 25%  | 25%  |
| Max+Max+Max         | 47%  | 42%  | 37%  | 37%  | 34%  | 32%  | 31%  | 31%  | 27%  | 26%  |
| Max+Max+Avg（较好）     | 42%  | 37%  | 33%  | 32%  | 30%  | 30%  | 30%  | 30%  | 26%  | 25%  |
| Avg+Avg+Avg  (略差）   | 50%  | 43%  | 40%  | 38%  | 36%  | 35%  | 34%  | 34%  | 31%  | 31%  |
| Avg+Max+Avg         | 47%  | 40%  | 37%  | 35%  | 34%  | 34%  | 32%  | 32%  | 31%  | 30%  |
| Avg+Max+Max（最差）     | 47%  | 42%  | 40%  | 38%  | 37%  | 36%  | 36%  | 36%  | 33%  | 33%  |

在提高精度的同时，可能引入噪声。

如果对不断重叠的Pooling结果，依旧全使用Max Pooling，那么有更大可能把噪声放大了。

而此时使用Avg Pooling就能把引入的噪声给降噪。

可以看到，Alex使用的Max+Avg+Avg无论在精度，还是在速度上，都远超其他组合。

从Avg Pooling角度来看，这玩意不仅计算量大，而且引入噪声，比Max Pooling不知差到哪里去了。

所以，凡是在Conv1使用Avg Pooling的，都没有好果子吃，精度太差。所以，没有重叠的Conv1更应该考虑使用Max Pooling。

在Conv2、Conv3中，使用Avg反而效果很好了，可能原因是抵消了重叠结构带来的部分噪声。两个Avg效果好于一个Avg，说明Avg也不是那么坏嘛。

### 3-非重叠结构

| Epoch/Pooling   | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |
| --------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Max+Avg+Avg(较好) | 52%  | 44%  | 39%  | 37%  | 35%  | 33%  | 32%  | 32%  | 29%  | 29%  |
| Max+Max+Max(最好） | 50%  | 40%  | 37%  | 35%  | 34%  | 33%  | 33%  | 32%  | 29%  | 29%  |
| Avg+Avg+Avg(最差) | 56%  | 48%  | 45%  | 41%  | 40%  | 38%  | 37%  | 36%  | 33%  | 33%  |
| Avg+Max+Max(较差) | 53%  | 45%  | 41%  | 39%  | 37%  | 37%  | 37%  | 36%  | 33%  | 33%  |

 

可以看到，此时Max+Max+Max在速度上领先了Max+Avg+Avg，这和重叠结构的情况相反。

对比了两个结构，不难发现，Avg在重叠结构里面作用巨大，而在非重叠结构里面，如果用Avg，则会影响网络训练。

除此之外，不难发现，非重叠结构的精度明显不如重叠结构，这也是为什么Alex论文贴的那张图比较奇葩了。